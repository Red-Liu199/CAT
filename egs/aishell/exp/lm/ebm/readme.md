## Train EBM with NNCE
We use the [LM pipeline](../../README.md) to train the energy based lanuage model with [NCE](https://proceedings.mlr.press/v9/gutmann10a).
```
python utils/pipeline/lm.py exp/lm/debm --ngpu 4
```
The pipeline includes 4 stages:
```
(data prepare) ->
tokenizer training -> data packing -> nn training -> inference
```

### Notes

* **In stage 2 (data packing)**, if you use a `PretrainedTokenizer` of type `BertTokenizer` to tokenize the data, the start token *[CLS]* and end token *[SEP]* will be added at the beginning and end of each sentence automatically. This is incompatible with the pipeline since the pipeline will automatically add another start token *0* at the beginning. So we need to delete the duplicated start token after packing data
```
python utils/reprocess.py exp/[your_exp]/lmbin exp/[your_exp]/lmbin --head_del 1
```
* The structure of the EBM is specified in [config_ebm.json](./config_ebm.json). Besides, we need to use an additional language model to generate noise samples, whose structure is specified in [config_noise.json](./config_noise.json).
* **Before stage 3 (nn training)**, we need a trained language model to initialize the noise model. We use a GPT-2 fine-tuned on aishell. The checkpoint path to be loaded is specified in `config['decoder']['kwargs']['check_noise_model']`.

### Result
The trained EBM is capable of rescoring ASR outputs. We rescore two N-best lists generated by [aishell-rnnt-v19](../../../exp/rnnt/rnnt-v19-torchaudio/readme.md) (in-domain testing) and [wenet-rnnt-v4](../../../../wenetspeech/exp/rnnt/rnnt-v4/readme.md) (cross-domain testing)
```
in-domain cer: 3.19
cross-domain cer: 3.36
```

|     training process    |
|:-----------------------:|
|![monitor](./monitor.png)|
