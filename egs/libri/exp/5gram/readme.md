### Basic info

### Appendix

* 5-gram model
* lm-a: trained on transcript data, 489MB on disk
* lm-b: trained on transcript + librispeech corpus, 15GB on disk

### Result
```
Perplexity over dataset is 
lm-a:
    word piece level: 35.95
    word level: 343.51

lm-b:
    word piece level: 25.58
    word level: 197.32
```

### Monitor figure
![monitor](./monitor.png)
